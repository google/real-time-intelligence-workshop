{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the examples on Dataflow\n",
    "\n",
    "This notebook illustrates a pipeline to stream the raw data from pub/sub to bigquery using dataflow runner and interactive runner .\n",
    "\n",
    "This pipeline processes the raw data from pub/sub and loads into Bigquery and in parallel it also windows the raw data (using fixed windowing) for every 3 seconds and calculates the mean of sensor values on the windowed data\n",
    "\n",
    "\n",
    "Note that running this example incurs a small [charge](https://cloud.google.com/dataflow/pricing) from Dataflow.\n",
    "\n",
    "Let's make sure the dependencies are installed. This allows to load the bq query results to a dataframe to plot the anomalies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you do `pip install db-dtypes` restart the kernel by clicking on the reload icon up top near the navigation menu. Once restarted, proceed with the rest of the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets make sure the Dataflow API is enabled. This [allows](https://cloud.google.com/apis/docs/getting-started#enabling_apis) your project to access the Dataflow service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable dataflow.googleapis.com\n",
    "!gcloud services enable dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start with necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "import random\n",
    "import time\n",
    "from google.cloud import pubsub_v1,bigquery\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive import interactive_runner\n",
    "from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys,Create,Map , CombineGlobally ,dataframe\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import google.auth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "publisher  = pubsub_v1.PublisherClient() #Pubsub publisher client\n",
    "subscriber = pubsub_v1.SubscriberClient() #Pubsub subscriber client\n",
    "client     = bigquery.Client() #bigquery client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set the variables . These variables will be referenced in later sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_project=!gcloud config get-value project\n",
    "project_id=dest_project[1]\n",
    "print(project_id)\n",
    "\n",
    "pubsub_topic                       = project_id + \"-\" + \"topic\" \n",
    "pubsub_subscription                = pubsub_topic + \"-\" + \"sub\"\n",
    "pubsub_topic_path                  = publisher.topic_path(project_id, pubsub_topic)\n",
    "pubsub_subscription_path           = subscriber.subscription_path(project_id, pubsub_subscription)\n",
    "\n",
    "bq_dataset                         = \"anomaly_detection_demo\"\n",
    "bigquery_agg_schema                = \"sensorID:STRING,sensorValue:FLOAT,windowStart:DATETIME,windowEnd:DATETIME\"\n",
    "bigquery_raw_schema                = \"sensorID:STRING,timeStamp:DATETIME,sensorValue:FLOAT\"\n",
    "bigquery_raw_table                 = bq_dataset + \".anomaly_raw_table\" \n",
    "bigquery_agg_table                 = bq_dataset + \".anomaly_windowed_table\" \n",
    "region                             = \"us-central1\"\n",
    "bucket_name                        = project_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Create Pub/sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {pubsub_topic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error that says `Run client channel backup poller: UNKNOWN:pollset_` don't be alarmed it won't effect the job. It is just a formatting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Create Pub/sub subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub subscriptions create {pubsub_subscription} --topic={pubsub_topic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create BigQuery Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location={region} mk --dataset {project_id}:{bq_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create BigQuery Tables\n",
    "\n",
    "raw big query schema\n",
    "\n",
    "![raw-schema](Images/raw-schema.png)\n",
    "\n",
    "aggregated big query schema\n",
    "![agg-schema](Images/agg-schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq mk --schema {bigquery_raw_schema} -t {bigquery_raw_table}\n",
    "!bq mk --schema {bigquery_agg_schema} -t {bigquery_agg_table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error that says `Run client channel backup poller: UNKNOWN:pollset_` don't be alarmed it won't effect the job. It is just a formatting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create GCS Bucket  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -c standard -l {region} gs://{bucket_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. IMPORTANT! open GCS bucket from console and create a folder called dataflow.\n",
    "path should be  gs://project_id/dataflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. set the pipeline options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(flags={})\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "options.view_as(GoogleCloudOptions).project = project_id\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow runs.\n",
    "options.view_as(GoogleCloudOptions).region = region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. create the function to format the raw data and processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add window begin datetime and endtime to the aggregated PCollections.\n",
    "class FormatDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        from datetime import datetime\n",
    "        window_start = datetime.fromtimestamp(window.start)\n",
    "        window_end = datetime.fromtimestamp(window.end)\n",
    "        return [{\n",
    "        'sensorID': element[0],\n",
    "        'sensorValue': element[1],\n",
    "        'windowStart': window_start,\n",
    "        'windowEnd': window_end\n",
    "        }]                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the raw PCollections\n",
    "class ProcessDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield element "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Construct the pipeline \n",
    "This step will take the pipeline from pub/sub topic and do some processing. It will process the raw data into raw PCollections and process the aggregated windowed data into aggregated pcollections.\n",
    "\n",
    "With the aggregated window, the pipeline will read the data from the pub/topic and group the data into 5 sec intervals. Lastly it will calculate the mean of sensor value for each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fixed-window](Images/fixed-window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pipeline options \n",
    "p = beam.Pipeline(interactive_runner.InteractiveRunner(), options=options)\n",
    "\n",
    "# pub/sub => mapped(pcollections)\n",
    "mapped   = (p  | \"ReadFromPubSub\" >> beam.io.gcp.pubsub.ReadFromPubSub(subscription=pubsub_subscription_path)\n",
    "               | \"Json Loads\" >> Map(json.loads))\n",
    "\n",
    "# mapped(input pcollections => raw_data(output pcollections)\n",
    "raw_data = (mapped \n",
    "               | 'Format' >> beam.ParDo(ProcessDoFn()))\n",
    "\n",
    "# mapped(input pcollections) => agg_date(output pcollections)            \n",
    "agg_data = (mapped \n",
    "               | \"Map Keys\" >> Map(lambda x: (x[\"SensorID\"],x[\"SensorValue\"]))\n",
    "               | \"ApplyFixedWindow\" >> beam.WindowInto(beam.window.FixedWindows(5))\n",
    "               | \"Total Per Key\" >> beam.combiners.Mean.PerKey()\n",
    "               | 'Final Format' >> beam.ParDo(FormatDoFn()))                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Pipeline` is constructed by an `InteractiveRunner`, so you can use operations such as `ib.collect` or `ib.show`.\n",
    "### Important \n",
    "Run steps 1-4 in simulator script(PythonSimulator.ipynb) in a separate tab -- (this is to simulate the data and writes to pub/sub topic to test interactiverunner(1 message per millisecond until it reaches 100 messages)\n",
    "\n",
    "Remember to **only** run steps 1-4 for now. We will come back to this script to run step 5 later.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(agg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.Dataflow Additions\n",
    "\n",
    "Now, for something a bit different. Because Dataflow executes in the cloud, you need to output to a cloud sink. In this case, you are loading the transformed data into Cloud Storage.\n",
    "\n",
    "First, set up the `PipelineOptions` to specify to the Dataflow service the Google Cloud project, the region to run the Dataflow Job, and the SDK location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Adjust the following to choose a Cloud Storage location.\n",
    "dataflow_gcs_location = \"gs://<add your project id>/dataflow\"\n",
    "\n",
    "# Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary.\n",
    "# options.view_as(GoogleCloudOptions).staging_location = dataflow_gcs_location\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow job name. when pipeline runs as dataflowrunner.\n",
    "options.view_as(GoogleCloudOptions).job_name = project_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the bigquery table to write `add_data` to,\n",
    "# based on the `bigquery_raw_table` variable set earlier.\n",
    "(raw_data | 'Write raw data to Bigquery' \n",
    " >> beam.io.WriteToBigQuery(\n",
    "                bigquery_raw_table,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n",
    "# Specifying the bigquery table to write `add_data` to,\n",
    "# based on the `bigquery_agg_table` variable set earlier.\n",
    "(agg_data | 'Write windowed aggregated data to Bigquery' \n",
    " >> beam.io.WriteToBigQuery(\n",
    "                bigquery_agg_table,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Ensure that the graph is correct before sending it out to Dataflow.\n",
    "# Because this is a notebook environment, unintended additions to the graph may have occurred when rerunning cells. \n",
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.Running the pipeline\n",
    "\n",
    "Now you are ready to run the pipeline on Dataflow. `run_pipeline()` runs the pipeline and return a pipeline result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important \n",
    "![dataflowStatus](Images/dataflowFailed.png)\n",
    "\n",
    "Before moving forward, check the dataflow job to see if it's running (Hamburger menu->Dataflow->Jobs). If the status shows as `failed`, **rerun** the above cell `pipeline_result = DataflowRunner().run_pipeline(p, options=options)` one more time. This happens when the Dataflow API is not fully enabled. It takes a minute or so for the API to permeate fully.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pipeline_result` handle, the following code builds a link to the Google Cloud Console web page that shows you details of the Dataflow job you just started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtaflow job\n",
    "![dataflow-job](Images/DataflowJob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important \n",
    "Run step5 in simulator script(PythonSimulator.ipynb) that is in a separate tab -- (this is to simulate the data and writes to pub/sub topic to test dataflow runner(1 message per millisecond until it reaches 5000 messages). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Checking the raw table results (note: it will take ~90sec to appear the initial data in table due to dataflow warmup time)\n",
    "raw table results\n",
    "![raw-table-results](Images/raw-data-results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the raw data in BQ raw Table\n",
    "sql = 'SELECT * FROM `{}` '.format(bigquery_raw_table)\n",
    "query_job = client.query(sql)  # API request\n",
    "raw_df = query_job.to_dataframe()\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.Checking the agg table results\n",
    "agg table results\n",
    "![agg-table-results](Images/agg-data-results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the agg data in BQ raw Table\n",
    "sql = 'SELECT   sensorID , case when sensorValue >= 200 then \"Anomaly\" else \"Normal\" end as type, sensorValue,row_number() over (order by windowStart) as cycle FROM `{}` '.format(bigquery_agg_table)\n",
    "query_job = client.query(sql)  # API request\n",
    "agg_df = query_job.to_dataframe()\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.Plot the results in a simple scatterplot chart \n",
    "\n",
    "Chart will display Anomalies in red color and Normal in Green color\n",
    "![plot](Images/plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=['green' if g=='Normal' else 'red' for g in agg_df['type']]\n",
    "agg_df.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"cycle\",\n",
    "    y=\"sensorValue\"  , c = c, s = 150,\n",
    "    figsize=(20, 10)    \n",
    "    )\n",
    "plt.axhline(y=200, color='black', linestyle='-',linewidth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!!\n",
    "End of lab\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.45.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.45.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
